#!/usr/bin/env python3
"""
LLM Analyzer - Use Claude API to detect implicit tasks in documents.

This module analyzes documents to find:
- Implicit tasks (not marked with - [ ] but should be done)
- Incomplete sections
- Unanswered questions
- Follow-up items
- Draft/WIP content
"""

import json
import os
import sys
from pathlib import Path
from typing import TypedDict

# Try to import anthropic
try:
    import anthropic
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False


class ImplicitTask(TypedDict):
    """An implicit task detected by LLM analysis."""
    task: str
    reason: str
    confidence: str  # "high", "medium", "low"
    source_text: str  # Original text that triggered this detection


class AnalysisResult(TypedDict):
    """Result of LLM document analysis."""
    implicit_tasks: list[ImplicitTask]
    incomplete_sections: list[str]
    unanswered_questions: list[str]
    summary: str


# System prompt for task analysis
ANALYSIS_PROMPT = """„ÅÇ„Å™„Åü„ÅØ„Éâ„Ç≠„É•„É°„É≥„ÉàÂàÜÊûê„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Åß„Åô„ÄÇ
‰∏é„Åà„Çâ„Çå„Åü„Éâ„Ç≠„É•„É°„É≥„Éà„ÇíÂàÜÊûê„Åó„ÄÅÊöóÈªôÁöÑ„Å™„Çø„Çπ„ÇØ„ÇÑÊú™ÂÆåÊàê„ÅÆÁÆáÊâÄ„ÇíÊ§úÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

‰ª•‰∏ã„ÇíÊé¢„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
1. **ÊöóÈªôÁöÑ„Å™„Çø„Çπ„ÇØ**: ÊòéÁ§∫ÁöÑ„Å´„Äå- [ ]„Äç„Å®„Éû„Éº„ÇØ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åå„ÄÅ„ÇÑ„Çã„Åπ„Åç„Åì„Å®„ÇíÁ§∫ÂîÜ„Åô„ÇãÊñá
   - ‰æã: „ÄåÂæå„ÅßÁ¢∫Ë™ç„Åô„Çã„Äç„ÄåË¶ÅÊ§úË®é„Äç„Äå„Äú„ÅåÂøÖË¶Å„Äç„Äå„Äú„Åô„Åπ„Åç„Äç
2. **Êú™ÂÆåÊàê„Çª„ÇØ„Ç∑„Éß„É≥**: Á©∫„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÄÅ„ÄåTBD„Äç„ÄåWIP„Äç„ÄåDraft„Äç„Å™„Å©„ÅÆË®òËºâ
3. **Êú™ÂõûÁ≠î„ÅÆË≥™Âïè**: „ÄåÔºü„Äç„ÅßÁµÇ„Çè„ÇãÊñá„ÅßÂõûÁ≠î„Åå„Å™„ÅÑ„ÇÇ„ÅÆ
4. **„Éï„Ç©„É≠„Éº„Ç¢„ÉÉ„ÉóÈ†ÖÁõÆ**: „ÄåÊ¨°Âõû„Äç„ÄåÊòéÊó•„Äç„ÄåÊù•ÈÄ±„Äç„Å™„Å©„ÅÆÊôÇÈñìË°®Áèæ„ÇíÂê´„ÇÄ„Ç¢„ÇØ„Ç∑„Éß„É≥

JSONÂΩ¢Âºè„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
```json
{
  "implicit_tasks": [
    {
      "task": "„Çø„Çπ„ÇØ„ÅÆÂÜÖÂÆπ",
      "reason": "„Å™„Åú„Åì„Çå„Åå„Çø„Çπ„ÇØ„Å®Âà§Êñ≠„Åó„Åü„Åã",
      "confidence": "high/medium/low",
      "source_text": "ÂÖÉ„ÅÆ„ÉÜ„Ç≠„Çπ„Éà"
    }
  ],
  "incomplete_sections": ["„Çª„ÇØ„Ç∑„Éß„É≥Âêç1", "„Çª„ÇØ„Ç∑„Éß„É≥Âêç2"],
  "unanswered_questions": ["Ë≥™Âïè1", "Ë≥™Âïè2"],
  "summary": "„Éâ„Ç≠„É•„É°„É≥„ÉàÂÖ®‰Ωì„ÅÆÂÆåÊàêÂ∫¶„Å´„Å§„ÅÑ„Å¶„ÅÆÁü≠„ÅÑË¶ÅÁ¥Ñ"
}
```

Ê≥®ÊÑèÔºö
- Êó¢„Å´„Äå- [ ]„Äç„ÇÑ„Äå- [x]„Äç„Åß„Éû„Éº„ÇØ„Åï„Çå„Å¶„ÅÑ„Çã„Çø„Çπ„ÇØ„ÅØÈô§Â§ñ
- TODO:, FIXME: „Å™„Å©ÊòéÁ§∫ÁöÑ„Å™„Éû„Éº„Ç´„Éº„ÇÇÈô§Â§ñÔºàÂà•ÈÄîÂá¶ÁêÜ„Åï„Çå„ÇãÔºâ
- Á¢∫‰ø°Â∫¶„Åå‰Ωé„ÅÑ„ÇÇ„ÅÆ„ÇÇÂê´„ÇÅ„Å¶OKÔºà„É¶„Éº„Ç∂„Éº„ÅåÂà§Êñ≠„Åß„Åç„Çã„Çà„ÅÜ„Å´Ôºâ
"""


class LLMAnalyzer:
    """Analyze documents using Claude API to detect implicit tasks."""

    def __init__(
        self,
        api_key: str | None = None,
        model: str = "claude-sonnet-4-20250514",
        use_feedback: bool = True,
    ):
        """
        Initialize the LLM analyzer.

        Args:
            api_key: Anthropic API key. If None, uses ANTHROPIC_API_KEY env var.
            model: Model to use for analysis.
            use_feedback: Whether to include feedback examples in prompts.
        """
        if not HAS_ANTHROPIC:
            raise ImportError(
                "anthropic package is required for LLM analysis. "
                "Install with: pip install anthropic"
            )

        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError(
                "API key required. Set ANTHROPIC_API_KEY environment variable "
                "or pass api_key parameter."
            )

        self.model = model
        self.client = anthropic.Anthropic(api_key=self.api_key)
        self.use_feedback = use_feedback
        self._feedback_context = ""

        # Load feedback examples if enabled
        if use_feedback:
            self._load_feedback_examples()

    def _load_feedback_examples(self):
        """Load feedback examples for few-shot learning."""
        try:
            from feedback import get_store, format_examples_for_prompt

            store = get_store()
            stats = store.get_stats()

            # Only use feedback if we have enough examples
            if stats["total"] >= 3:
                examples = store.get_balanced_examples(count_per_type=3)
                self._feedback_context = format_examples_for_prompt(examples)

                # Add stats context
                self._feedback_context += f"\n\n## Feedback Statistics:\n"
                self._feedback_context += f"- Total feedback: {stats['total']}\n"
                self._feedback_context += f"- Acceptance rate: {stats['acceptance_rate']:.1%}\n"
                self._feedback_context += f"- Missed tasks reported: {stats['missed']}\n"

                if stats["acceptance_rate"] < 0.5:
                    self._feedback_context += (
                        "\nNote: Low acceptance rate suggests being more conservative "
                        "with task detection. Focus on high-confidence detections.\n"
                    )

                if stats["missed"] > 0:
                    # Calculate recall issue ratio
                    recall_ratio = stats["missed"] / (stats["accepted"] + stats["missed"]) if (stats["accepted"] + stats["missed"]) > 0 else 0
                    if recall_ratio > 0.1:
                        self._feedback_context += (
                            f"\nIMPORTANT: {stats['missed']} tasks were missed (not detected). "
                            "Be more aggressive in detecting implicit tasks. "
                            "Look carefully at the MISSED examples above and detect similar patterns.\n"
                        )
        except Exception as e:
            # Feedback not available, continue without it
            print(f"Note: Feedback not available: {e}", file=sys.stderr)

    def analyze_document(self, content: str, file_name: str = "") -> AnalysisResult:
        """
        Analyze a document to detect implicit tasks.

        Args:
            content: The document content to analyze.
            file_name: Optional file name for context.

        Returns:
            AnalysisResult with detected implicit tasks and issues.
        """
        if not content.strip():
            return AnalysisResult(
                implicit_tasks=[],
                incomplete_sections=[],
                unanswered_questions=[],
                summary="Empty document"
            )

        # Truncate very long documents
        max_chars = 8000
        if len(content) > max_chars:
            content = content[:max_chars] + "\n\n[...truncated...]"

        user_message = f"„Éï„Ç°„Ç§„É´: {file_name}\n\n```markdown\n{content}\n```"

        # Build system prompt with feedback context if available
        system_prompt = ANALYSIS_PROMPT
        if self._feedback_context:
            system_prompt += f"\n\n# User Feedback History\n{self._feedback_context}"

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=2000,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_message}
                ]
            )

            # Parse the response
            response_text = response.content[0].text

            # Extract JSON from response
            result = self._parse_response(response_text)
            return result

        except anthropic.APIError as e:
            print(f"API error: {e}", file=sys.stderr)
            return self._empty_result(f"API error: {e}")
        except Exception as e:
            print(f"Analysis error: {e}", file=sys.stderr)
            return self._empty_result(f"Error: {e}")

    def _parse_response(self, response_text: str) -> AnalysisResult:
        """Parse the LLM response into AnalysisResult."""
        try:
            # Try to extract JSON from the response
            # Handle case where JSON is wrapped in ```json ... ```
            if "```json" in response_text:
                start = response_text.find("```json") + 7
                end = response_text.find("```", start)
                json_str = response_text[start:end].strip()
            elif "```" in response_text:
                start = response_text.find("```") + 3
                end = response_text.find("```", start)
                json_str = response_text[start:end].strip()
            else:
                # Try to find JSON object directly
                start = response_text.find("{")
                end = response_text.rfind("}") + 1
                json_str = response_text[start:end]

            data = json.loads(json_str)

            # Convert to proper types
            implicit_tasks = []
            for task in data.get("implicit_tasks", []):
                implicit_tasks.append(ImplicitTask(
                    task=task.get("task", ""),
                    reason=task.get("reason", ""),
                    confidence=task.get("confidence", "medium"),
                    source_text=task.get("source_text", "")
                ))

            return AnalysisResult(
                implicit_tasks=implicit_tasks,
                incomplete_sections=data.get("incomplete_sections", []),
                unanswered_questions=data.get("unanswered_questions", []),
                summary=data.get("summary", "")
            )

        except json.JSONDecodeError as e:
            print(f"Failed to parse LLM response: {e}", file=sys.stderr)
            print(f"Response was: {response_text[:500]}", file=sys.stderr)
            return self._empty_result("Failed to parse response")

    def _empty_result(self, summary: str = "") -> AnalysisResult:
        """Return an empty analysis result."""
        return AnalysisResult(
            implicit_tasks=[],
            incomplete_sections=[],
            unanswered_questions=[],
            summary=summary
        )

    def record_feedback(
        self,
        task: ImplicitTask,
        feedback: str,
        source_file: str = "",
        modified_text: str | None = None,
        reason: str | None = None,
    ):
        """
        Record user feedback for a detected task.

        Args:
            task: The ImplicitTask that was shown to user.
            feedback: "accepted", "rejected", or "modified".
            source_file: File where task was detected.
            modified_text: User's modified version (if feedback is "modified").
            reason: Reason for rejection (if feedback is "rejected").
        """
        try:
            from feedback import get_store

            store = get_store()
            store.add_feedback(
                task_text=task["task"],
                feedback=feedback,
                source_text=task.get("source_text", ""),
                source_file=source_file,
                modified_text=modified_text,
                reason=reason,
                confidence=task.get("confidence", "medium"),
            )
        except Exception as e:
            print(f"Failed to record feedback: {e}", file=sys.stderr)


def analyze_file(file_path: Path, api_key: str | None = None) -> AnalysisResult:
    """
    Convenience function to analyze a file.

    Args:
        file_path: Path to the file to analyze.
        api_key: Optional API key.

    Returns:
        AnalysisResult with detected issues.
    """
    if not file_path.exists():
        return AnalysisResult(
            implicit_tasks=[],
            incomplete_sections=[],
            unanswered_questions=[],
            summary="File not found"
        )

    try:
        content = file_path.read_text(encoding="utf-8")
    except Exception as e:
        return AnalysisResult(
            implicit_tasks=[],
            incomplete_sections=[],
            unanswered_questions=[],
            summary=f"Error reading file: {e}"
        )

    analyzer = LLMAnalyzer(api_key=api_key)
    return analyzer.analyze_document(content, file_path.name)


def main():
    """CLI entry point for testing."""
    import argparse

    parser = argparse.ArgumentParser(description="Analyze documents for implicit tasks")
    parser.add_argument("file", type=Path, help="File to analyze")
    parser.add_argument("--api-key", help="Anthropic API key (or set ANTHROPIC_API_KEY)")

    args = parser.parse_args()

    try:
        result = analyze_file(args.file, args.api_key)

        print(f"\nüìÑ Analysis of: {args.file.name}")
        print("=" * 50)

        if result["implicit_tasks"]:
            print(f"\nüîç Implicit Tasks ({len(result['implicit_tasks'])}):")
            for task in result["implicit_tasks"]:
                conf_icon = {"high": "üî¥", "medium": "üü°", "low": "üü¢"}.get(task["confidence"], "‚ö™")
                print(f"  {conf_icon} {task['task']}")
                print(f"      Reason: {task['reason']}")
                if task["source_text"]:
                    print(f"      Source: \"{task['source_text'][:50]}...\"")

        if result["incomplete_sections"]:
            print(f"\nüìù Incomplete Sections ({len(result['incomplete_sections'])}):")
            for section in result["incomplete_sections"]:
                print(f"  - {section}")

        if result["unanswered_questions"]:
            print(f"\n‚ùì Unanswered Questions ({len(result['unanswered_questions'])}):")
            for q in result["unanswered_questions"]:
                print(f"  - {q}")

        print(f"\nüìä Summary: {result['summary']}")

    except ImportError as e:
        print(f"Error: {e}", file=sys.stderr)
        print("Install with: pip install anthropic", file=sys.stderr)
        sys.exit(1)
    except ValueError as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
